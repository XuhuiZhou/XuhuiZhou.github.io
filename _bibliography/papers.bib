---
---
@misc{zhou2024haicosystemecosystemsandboxingsafety,
      title={HAICOSYSTEM: An Ecosystem for Sandboxing Safety Risks in Human-AI Interactions}, 
      author={Xuhui Zhou and Hyunwoo Kim and Faeze Brahman and Liwei Jiang and Hao Zhu and Ximing Lu and Frank Xu and Bill Yuchen Lin and Yejin Choi and Niloofar Mireshghallah and Ronan Le Bras and Maarten Sap},
      year={2024},
      eprint={2409.16427},
      abbr={arXiv},
      archivePrefix={arXiv},
      booktitle={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2409.16427}, 
}

@misc{su2024ailiedarexaminetradeoffutility,
      title={AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM Agents}, 
      author={Zhe Su and Xuhui Zhou and Sanketh Rangreji and Anubha Kabra and Julia Mendelsohn and Faeze Brahman and Maarten Sap},
      year={2024},
      eprint={2409.09013},
      abbr={arXiv},
      archivePrefix={arXiv},
      booktitle={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2409.09013}, 
}

@misc{fan2024userdrivenvaluealignmentunderstanding,
      title={User-Driven Value Alignment: Understanding Users' Perceptions and Strategies for Addressing Biased and Discriminatory Statements in AI Companions}, 
      author={Xianzhe Fan and Qing Xiao and Xuhui Zhou and Jiaxin Pei and Maarten Sap and Zhicong Lu and Hong Shen},
      year={2024},
      eprint={2409.00862},
      abbr={arXiv},
      archivePrefix={arXiv},
      booktitle={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2409.00862}, 
}

@misc{huang2024resiliencemultiagentsystemsmalicious,
      title={On the Resilience of Multi-Agent Systems with Malicious Agents}, 
      author={Jen-tse Huang and Jiaxu Zhou and Tailin Jin and Xuhui Zhou and Zixi Chen and Wenxuan Wang and Youliang Yuan and Maarten Sap and Michael R. Lyu},
      year={2024},
      eprint={2408.00989},
      abbr={arXiv},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2408.00989}, 
}

@inproceedings{consent2024,
  title={Consent in Crisis: The Rapid Decline of the AI Data Commons},
  author       = {Shayne Longpre1, Robert Mahari1, Ariel Lee1, Campbell Lund1, Hamidah Oderinwale2,
William Brannon2, Nayan Saxena2, Naana Obeng-Marnu2, Tobin South2, Cole Hunter2, Kevin Klyman2, Christopher Klamm2, Hailey Schoelkopf2, Nikhil Singh2, Manuel Cherep2, Ahmad Mustafa Anis3, An Dinh3, Caroline Chitongo3, Da Yin3, Damien Sileo3, Deividas Mataciunas3, Diganta Misra3, Emad Alghamdi3, Enrico Shippole3, Jianguo Zhang3, Joanna Materzynska3, Kun Qian3, Kush Tiwary3, Lester Miranda3, Manan Dey3, Minnie Liang3, Mohammed Hamdy3, Niklas Muennighoff3, Seonghyeon Ye3, Seungone Kim3, Shrestha Mohanty3, Vipul Gupta3, Vivek Sharma3, Vu Minh Chien3, Xuhui Zhou3, Yizhi Li3, Caiming Xiong4, Luis Villa4,
Stella Biderman4, Hanlin Li4, Daphne Ippolito4, Sara Hooker4, Jad Kabbara4, and Sandy Pentland},
  abbr = {NeurIPS},
  year = {2024},
  booktitle = {NeurIPS},
  url = {https://www.dataprovenance.org/Consent_in_Crisis.pdf},
  website = {https://www.dataprovenance.org/},
}

@inproceedings{jain2024polyglotoxicitypromptsmultilingualevaluationneural,
      title={PolygloToxicityPrompts: Multilingual Evaluation of Neural Toxic Degeneration in Large Language Models}, 
      author={Devansh Jain and Priyanshu Kumar and Samuel Gehman and Xuhui Zhou and Thomas Hartvigsen and Maarten Sap},
      year={2024},
      eprint={2405.09373},
      abbr={COLM},
      booktitle={COLM},
      primaryClass={cs.CL},
      url={https://colmweb.org/}, 
}

@inproceedings{zhou2024reallifejustfantasy,
      title={Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs}, 
      author={Xuhui Zhou and Zhe Su and Tiwalayo Eisape and Hyunwoo Kim and Maarten Sap},
      year={2024},
      eprint={2403.05020},
      abbr={EMNLP},
      booktitle={EMNLP},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.05020}, 
}

@inproceedings{Niloofar2024CanLLMsKeepASecret,
title={Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory},
author={Mireshghallah, Niloofar and Kim, Hyunwoo and Zhou, Xuhui and Tsvetkov, Yulia and Sap, Maarten and Shokri, Reza and Choi, Yejin},
abbr={ICLR},
booktitle={ICLR},
bibtex_show={true},
year={2024},
url={https://openreview.net/forum?id=gmg7t8b4s0},
website={https://confaide.github.io/},
award="Spotlight (top 5%)",
}

@inproceedings{zhou2024sotopia,
title={SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents},
author={Zhou*, Xuhui and Zhu*, Hao
and
Mathur, Leena and Zhang, Ruohong and Qi, Zhengyang and Yu, Haofei
and
Morency, Louis-Philippe and Bisk, Yonatan and Fried, Daniel and Neubig, Graham and Sap, Maarten},
abbr={ICLR},
booktitle={ICLR},
bibtex_show={true},
year={2024},
url={https://openreview.net/forum?id=mM7VurbA4r},
award="Spotlight (top 5%)",
website={https://www.sotopia.world/}
}

@inproceedings{zhou2024arena,
title={WebArena: A Realistic Web Environment for Building Autonomous Agents},
author={Zhou, Shuyan and Xu, Frank F. and Zhu, Hao and Zhou, Xuhui and Lo, Robert and Sridhar, Abishek and Cheng, Xianyi and Bisk, Yonatan and Fried, Daniel and Alon, Uri and Neubig, Graham},
abbr={ICLR},
booktitle={ICLR},
year={2024},
url={https://openreview.net/forum?id=oKn9c6ytLx},
bibtex_show={true},
website={https://webarena.dev/},
demo={https://webarena.dev/#try-it-yourself},
}


@inproceedings{shapira2024cleverHans,
  title={Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models},
  author={Shapira, Natalie and Levy, Mosh and Seyed Alavi, Hossein and Zhou, Xuhui and Choi, Yejin and Goldberg, Yoav and Sap, Maarten and Shwartz, Vered},
  year={2024},
  url={https://arxiv.org/abs/2305.14763},
  booktitle={EACL},
  abbr={EACL},
  bibtex_show={true},
}

@inproceedings{zhou2023cobra,
title={COBRA üêç Frames: Contextual Reasoning about Effects and Harms of Offensive Statements},
author={Zhou, Xuhui and Zhu, Hao and Yerukola, Akhila and Davidson, Thomas and D. Hwang, Jena and Swayamdipta, Swabha and Sap, Maarten},
abbr="ACL",
year={2023},
url={https://arxiv.org/abs/2306.01985},
bibtex_show={true},
booktitle={Findings of ACL},
website={https://cobra.xuhuiz.com/},
demo={https://cobra.allen.ai/},
selected={true},
}

@inproceedings{yerukola2023contextRewrite,
  title={``Don't Take This Out of Context!'' On the Need for Contextual Models and Evaluations for Stylistic Rewriting},
  author={Yerukola, Akhila and Zhou, Xuhui and Sap, Maarten},
  year={2023},
  booktitle={EMNLP},
  url={https://arxiv.org/abs/2305.14755},
  abbr={EMNLP},
  bibtex_show={true}
}

@inproceedings{kim2023fantom,
  title={FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions},
  author={Kim, Hyunwoo and Sclar, Melanie and Zhou, Xuhui and Le Bras, Ronan and Kim, Gunhee and Choi, Yejin and Sap, Maarten
},
  year={2023},
  booktitle={EMNLP},
  url={https://arxiv.org/abs/2310.15421},
  website={https://hyunw.kim/fantom/},
  abbr={EMNLP},
  bibtex_show={true}
}


@inproceedings{Downey2022LearningTT,
  title={Learning to translate by learning to communicate},
  author={C. Downey* and Xuhui Zhou* and L. Liu and Shane Steinert-Threlkeld},
  abbr="EMNLP",
  booktitle="EMNLP MRL",
  url={https://arxiv.org/abs/2207.07025},
  bibtex_show={true},
  year={2023}
}

@inproceedings{sap2022annotatorsWithAttitudes,
   title={Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection},
   author={Sap, Maarten and Swayamdipta, Swabha and Vianna, Laura and Zhou, Xuhui and Choi, Yejin and Smith, Noah A.},
   url={https://arxiv.org/abs/2111.07997},
   abbr="NACCL",
   year={2022},
   bibtex_show={true},
   booktitle={NAACL},
   journal={NAACL}
}

@inproceedings{zhilin2022exandinfpersonalatt,
   title={Extracting and Inferring Personal Attributes from Dialogue},
   author={Wang, Zhilin and Zhou, Xuhui and Koncel-Kedziorski, Rik and Marin, Alex and Xia, Fei},
   url={https://arxiv.org/abs/2109.12702},
   abbr="ACL ConvAI",
   year={2022},
   bibtex_show={true},
   booktitle={ACL ConvAI}
}

@inproceedings{steinert-threlkeld2022emergent,
title={Emergent Communication Fine-tuning ({EC}-{FT}) for Pretrained Language Models},
author={Steinert-Threlkeld, Shane and Zhou, Xuhui and Liu, Zeyu and Downey, C.M.},
booktitle={Emergent Communication Workshop at ICLR 2022},
bibtex_show={true},
abbr="ICLR EmeCom",
award="Runner-up Best Paper",
year={2022},
url={https://openreview.net/forum?id=SUqrM7WR7W5}
}


@inproceedings{zhou-etal-2020-debiasing,
    title = "Challenges in Automated Debiasing for Toxic Language Detection",
    author = "Zhou, Xuhui  and
      Sap, Maarten  and
      Swayamdipta, Swabha and
      Choi, Yejin and
      Smith, Noah A.",
    booktitle = "EACL",
    abbr="EACL",
    html = "https://www.aclweb.org/anthology/2021.eacl-main.274.pdf",
    code = "https://github.com/XuhuiZhou/Toxic_Debias",    
    year = "2021",
    bibtex_show={true},
    selected={true}, 
}

@inproceedings{li-etal-2020-linguistically,
    title = "Linguistically-Informed Transformations ({LIT}): A Method for Automatically Generating Contrast Sets",
    author = "Li, Chuanrong  and
      Shengshuo, Lin  and
      Liu, Zeyu  and
      Wu, Xinyi  and
      Zhou, Xuhui  and
      Steinert-Threlkeld, Shane",
    abbr="BlackboxNLP",
    booktitle = "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    html = "https://www.aclweb.org/anthology/2020.blackboxnlp-1.12",
    pages = "126--135",
    abstract = "Although large-scale pretrained language models, such as BERT and RoBERTa, have achieved superhuman performance on in-distribution test sets, their performance suffers on out-of-distribution test sets (e.g., on contrast sets). Building contrast sets often requires human-expert annotation, which is expensive and hard to create on a large scale. In this work, we propose a Linguistically-Informed Transformation (LIT) method to automatically generate contrast sets, which enables practitioners to explore linguistic phenomena of interests as well as compose different phenomena. Experimenting with our method on SNLI and MNLI shows that current pretrained language models, although being claimed to contain sufficient linguistic knowledge, struggle on our automatically generated contrast sets. Furthermore, we improve models{'} performance on the contrast sets by applying LIT to augment the training data, without affecting performance on the original data.",
}

@inproceedings{zhou-etal-2020-multilevel,
    title = "Multilevel Text Alignment with Cross-Document Attention",
    author = "Zhou, Xuhui  and
      Pappas, Nikolaos  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    abbr="EMNLP",
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    html = "https://www.aclweb.org/anthology/2020.emnlp-main.407",
    pages = "5012--5025",
    website= "https://xuhuizhou.github.io/Multilevel-Text-Alignment/",
    abstract = "Text alignment finds application in tasks such as citation recommendation and plagiarism detection. Existing alignment methods operate at a single, predefined level and cannot learn to align texts at, for example, sentence \textit{and} document levels. We propose a new learning approach that equips previously established hierarchical attention encoders for representing documents with a cross-document attention component, enabling structural comparisons across different levels (document-to-document and sentence-to-document). Our component is weakly supervised from document pairs and can align at multiple levels. Our evaluation on predicting document-to-document relationships and sentence-to-document relationships on the tasks of citation recommendation and plagiarism detection shows that our approach outperforms previously established hierarchical, attention encoders based on recurrent and transformer contextualization that are unaware of structural correspondence between documents.",
}
@inproceedings{zhou-etal-2020-rpd,
    title = "{RPD}: A Distance Function Between Word Embeddings",
    author = "Zhou, Xuhui  and
      Huang, Shujian  and
      Zheng, Zaixiang",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop",
    abbr="ACL SRW",
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    html = "https://www.aclweb.org/anthology/2020.acl-srw.7",
    doi = "10.18653/v1/2020.acl-srw.7",
    pages = "42--50",
    abstract = "It is well-understood that different algorithms, training processes, and corpora produce different word embeddings. However, less is known about the relation between different embedding spaces, i.e. how far different sets of em-beddings deviate from each other. In this paper, we propose a novel metric called Relative Pairwise Inner Product Distance (RPD) to quantify the distance between different sets of word embeddings. This unitary-invariant metric has a unified scale for comparing different sets of word embeddings. Based on the properties of RPD, we study the relations of word embeddings of different algorithms systematically and investigate the influence of different training processes and corpora. The results shed light on the poorly understood word embeddings and justify RPD as a measure of the distance of embedding space.",
}

@inproceedings{ZHOU2019EvaluatingCI,
  title={Evaluating Commonsense in Pre-trained Language Models},
  author={Xuhui Zhou and Y. Zhang and Leyang Cui and Dandan Huang},
  html={https://ojs.aaai.org//index.php/AAAI/article/view/6523},
  year={2020},
  volume={abs/1911.11931},
  abbr="AAAI",
  booktitle="Proceedings of the AAAI Conference on Artificial Intelligence, 34(05)", 
  selected={true},
}

@book{przibram1967letters,
  bibtex_show={true},
  title={Letters on wave mechanics},
  author={Einstein, Albert and Schr√∂dinger, Erwin and Planck, Max and Lorentz, Hendrik Antoon and Przibram, Karl},
  year={1967},
  publisher={Vision},
  preview={wave-mechanics.gif}
}
